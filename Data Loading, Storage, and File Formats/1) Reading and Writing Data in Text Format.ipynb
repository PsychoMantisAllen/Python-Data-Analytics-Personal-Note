{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas can read file from multiple formats, including:\n",
    "1. csv\n",
    "2. table\n",
    "3. fwf (fixed-width column format i.e., no delimiters)\n",
    "4. clipboard\n",
    "5. excel\n",
    "6. hdf (HDF5 file)\n",
    "7. html\n",
    "8. json (JavaScript Object Notation)\n",
    "9. msgpack (pandas data encoded using MessagePack binary format)\n",
    "10. pickle (an arbitary object stored in Python pickle format)\n",
    "11. sas\n",
    "12. sql\n",
    "13. stata\n",
    "14. feather\n",
    "\n",
    "Some functions like `pandas.read_csv` perform *type inference* because the column data types are not part of the data format.\n",
    "\n",
    "Using `pd.read_csv` in a comma-delimited data = `pd.read_table('file_path', sep = ',')`.\n",
    "\n",
    "When file doesn't have a header, you can leave it as default or specify the name using `header = None` or `names = []`.\n",
    "\n",
    "To specify a column to be the index of the returned DataFrame, use `index_col = 'the_column_name'`.\n",
    "\n",
    "To form a hierarchical index from multiple columns, use `index_col = ['column_1', 'column_2']`.\n",
    "\n",
    "If the fields are separated by whitespace, we can use `sep = '\\s+'` to delimit.\n",
    "\n",
    "`skiprows = []` can skip the specific rows.\n",
    "\n",
    "Missing Data is usually either not present (empty string) or marked by some *sentinel* value. (e.g. NA, NULL)\n",
    "1. use `pd.isnull()` to check missing data\n",
    "2. `na_values` option take a list or set of strings to consider missing value, use by `na_values = ['NULL']`\n",
    "3. different NA sentinels can be specified for each column in a dict: `dict1 = {'column1':['value1', 'value2'], 'column2':['value3']}`\n",
    "4. set `na_values = dict1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces\n",
    "\n",
    "If we don't want to look at the whole dataset every time we read the file, we can preset the pandas display settings:\n",
    "\n",
    "`pd.options.display.max_rows` or specify the row with `nrows = `\n",
    "\n",
    "To read a file in pieces, specify a *chunksize* as a number of rows `chunksize = `.\n",
    "\n",
    "The *TextParser* object returned by *read_csv* allows you to iterate over the parts of the file according to the *chunksize*. For example, we can iterate over **ex6.csv**, aggregating the value counts in the 'key' column like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chunker = pd.read_csv('ex6.csv', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allenyan/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:1: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tot = pd.Series([])\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value = 0)\n",
    "\n",
    "tot = tot.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E    368.0\n",
       "X    364.0\n",
       "L    346.0\n",
       "O    343.0\n",
       "Q    340.0\n",
       "M    338.0\n",
       "J    337.0\n",
       "F    335.0\n",
       "K    334.0\n",
       "H    330.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TextParser* is also equipped with a *get_chunk* method that enables you to read pieces of an arbitrary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to Text Format\n",
    "\n",
    "Using `to_csv` method, we can generate data out to a comma-separated file.\n",
    "\n",
    "Using `sys.stdout` to print text result to console (before that, remember to `import sys`); other delimiters, such as \"|\", can be used by `sep = '|'`.\n",
    "\n",
    "Missing values should be denoted by `na_rep = 'NULL'`.\n",
    "\n",
    "Disable row and column label display by `index = False` and `header = False`.\n",
    "\n",
    "These methods can apply to Series as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Delimited Formats\n",
    "\n",
    "For any file with a single-character delimiter, we can use *csv* built-in method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('ex7.csv')\n",
    "\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['1', '2', '3']\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ex7.csv') as f:\n",
    "    lines = list(csv.reader(f)) # read the file into a list of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, values = lines[0], lines[1:] # split lines into header and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {h: v for h, v in zip(header, zip(*values))} # zip transposes rows to columns\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a customized csv format, simply make a subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "\n",
    "reader = csv.reader(f, dialect = my_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For files with complicated delimiters, we cannot use *csv*. Rather we need to do line splitting and other cleansing such as string's *split* or regular expression `re.split`.\n",
    "\n",
    "`csv.writer` can write delimited files manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mydata.csv', 'w') as f:\n",
    "    writer = csv.writer(f, dialect = my_dialect)\n",
    "    writer.writerow(('X', 'X', 'X'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Data\n",
    "\n",
    "JSON - sending data by HTTP request between web browsers and other applications. To convert a JSON string to Python form, use `json.loads`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\", \n",
    " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"], \n",
    " \"pet\": null, \n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]}, \n",
    "              {\"name\": \"Katie\", \"age\": 38, \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}] \n",
    "} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Wes',\n",
       " 'places_lived': ['United States', 'Spain', 'Germany'],\n",
       " 'pet': None,\n",
       " 'siblings': [{'name': 'Scott', 'age': 30, 'pets': ['Zeus', 'Zuko']},\n",
       "  {'name': 'Katie', 'age': 38, 'pets': ['Sixes', 'Stache', 'Cisco']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result) # convert Python to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.read_json` converts JSON datasets into a Series or DataFrame. The default options is assuming each object in the JSON array is a row in the table.\n",
    "\n",
    "If export data from pandas to JSON, `to_json` could perform the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML and HTML: Web Scraping\n",
    "\n",
    "Libraries for HTML & XML:\n",
    "1. *lxml* - much faster in general\n",
    "2. *Beautiful Soup* & *html5lib* - handle malformed files better\n",
    "\n",
    "`pandas.read_html` by default searches for and attempts to parse all tabular data contained within *table* tags. Result is a list of DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing XML with lxml.objectify\n",
    "\n",
    "Using `lxml.objectify` we can parse the file and get a reference to the root node of the XML file with `getroot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "\n",
    "path = 'file path'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`root.INDICATOR` returns a generator yielding each `<INDICATOR>` XML element. For each record, we can populate a dict of tag names to data values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\n",
    "              'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
    "\n",
    "for elt in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML data can be much complicated, consider an HTML link tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "tag = '<a href=\"http://www.google.com\">Google</a>'\n",
    "root = objectify.parse(StringIO(tag)).getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can try `root.get('href')` and `root.text` to see what happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
