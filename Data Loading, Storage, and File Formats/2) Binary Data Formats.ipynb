{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy way to store data (*serialization*) is using built-in `pickle` serialization - `to_pickle`;\n",
    "\n",
    "read such file by `pandas.read_pickle`\n",
    "\n",
    "## ! \n",
    "`pickle` is only recommended for short-term storage - hard to guarantee the format to be stable over time (when library version changed, may switch to different format)\n",
    "\n",
    "pandas has built-in support for two more binary data formats: **HDF5** and **MessagePack**. Some other formats are:\n",
    "1. *bcolz* - A compressable column-oriented binary format based on the Blosc compression library.\n",
    "2. *Feather* - A cross-language column-oriented file format I designed with R programming community's Hadley Wickham. Feather uses the *Apache Arrow* columnar memory format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HDF5 Format\n",
    "\n",
    "HDF5 is well-known for storing large quantities of scientific array data. It is available as a C library. \"HDF\" stands for *hierarchical data format*.\n",
    "1. HDF5 file can store multiple datasets and supporting metadata.\n",
    "2. HDF5 supports on-the-fly compression with a variety of compression modes - enabling data with repeated patterns to be stored more efficiently.\n",
    "3. HDF5 is good for large datasets that don't fit into memory - efficiently read and write small sections of much larger arrays.\n",
    "\n",
    "We can access HDF5 files with *PyTables* or *h5py*, but pandas provides a **high-level interface** that simplifies storing Series and DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'a': np.random.randn(100)})\n",
    "store = pd.HDFStore('mydata.h5') \n",
    "# HDFStore works like a dict and handles the low-level details\n",
    "store['obj1'] = frame\n",
    "store['obj1_col'] = frame['a']\n",
    "# Objects contained in the HDF5 file can then \n",
    "# be retrieved with the same dict-like API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFStore supports 2 storage schemas - `fixed` and `table`. The latter is generally slower but supports query operations using a special syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put('obj2', frame, format = 'table')\n",
    "# put is an explicit version of store['obj2'] = frame method\n",
    "# but allows us to set other options like the storage format\n",
    "store.select('obj2', where = ['index >= 10 and index <= 15'])\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.read_hdf gives you a shortcut to these tools\n",
    "frame.to_hdf('mydata.h5', 'obj3', format = 'table')\n",
    "pd.read_hdf('mydata.h5', 'obj3', where = ['index < 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - If processing data that is stored on remote servers (e.g. Amazon S3 or HDFS), use a different binary format designed for distributed storage like [Apache Parquet](http://parquet.apache.org/) may be more suitable.\n",
    " - If working large dataset locally, PyTables and h5py may suit the needs. As many data analysis problems are I/O-bound (Not CPU-bound), using HDF5 can accelerate the applications.\n",
    " - HDF5 is not a database. It is **Write-once, read-many** datasets. While data can be added to a file at any time, if multiple writers do so simultaneously, the file can become corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra tip!** I/O bound (Input/Output) means the speed of reading and writing to disk, network, etc. While CPU bound means the speed of CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Microsoft Excel Files\n",
    "\n",
    "Use `ExcelFile` and `pandas.read_excel` to read excel file - these tolls use the add-on packages `xlrd` and `openpyxl` to read XLS and XLSX files. (Remeber to install!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile('ex1.xlsx')\n",
    "pd.read_excel(xlsx, 'Sheet1')\n",
    "# Data stored in a sheet can then be read into DataFrame with parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively we can pass the filename to pandas.read_excel\n",
    "frame = pd.read_excel('ex1.xlsx', 'Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write pandas data to Excel format, you must first create an `ExcelWriter`, then write data to it using pandas objects' `to_excel` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('ex2.xlsx')\n",
    "frame.to_excel(writer, 'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_excel('examples/ex2.xlsx') \n",
    "# pass a file path and avoid ExcelWriter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
